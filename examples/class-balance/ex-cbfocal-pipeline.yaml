data_type: float64
Dataset:
  datasets_path: "datasets/"
  dataset_name: "IMB_CIFAR10"
  dataset_params:
    imb_factor: 100
    normalize_mu:
      val1: 0.4914
      val2: 0.4822
      val3: 0.4465
    normalize_std:
      val1: 0.2023
      val2: 0.1994
      val3: 0.2010
    jitter_brightness: 0.4
    jitter_contrast: 0.4
    jitter_saturation: 0.4
    jitter_hue: 0.25
DataGeneration:
  batch_size: 128
  num_workers: 2
  pad: 4
  image_size:
    width: 32
    height: 32
  train_shuffle: True
  sampler: None
  plotting:
    draw_dataset_plots: True
    plot_size:
      width: 8
      height: 6
    plot_path: "output/dataset_plots/"
  train_transform:
    -
      transform_name: "pad"
      transform_params:
        fill: 0
        mode: "constant"
    -
      transform_name: "random_resized_crop"
      transform_params: None
    -
      transform_name: "random_horizontal_flip"
      transform_params: None
  test_transform:
    -
      transform_name: "center_crop"
      transform_params: None
Training:
  epoch_count: 10
  multi_gpu: False
  backup:
    save_models: True  # If True, save models' states after training is finished.
    save_epoch_interval: 2  # If save_models is true, save models also every X epochs. No save during training if X < 0.
    models_path: "output/models/"  # Mandatory field only if save_models or load_models is True
    load_models: False  # If True, initialize each model configuration by loading the latest saved model state in models_path
  tasks:
    # TODO: How to handle class balancing? old cb_focal and focal now redirect to the same class.
    #   Besides, class balancing weights are computed under a datasets.py function...
    -  # Without class balancing
      model_name: "ResNet32"
      loss_name: "FocalLoss"
      loss_args:
        gamma: 0.5
      task_options:
        init_fc_bias: True
    -  # With class balancing
      model_name: "ResNet32"
      loss_name: "FocalLoss"
      loss_args:
        gamma: 0.5
      task_options:
        init_fc_bias: True
        cb_beta: 0.999
  optimizer:
    # name: "sgd_linear_warmup" # TODO
    name: "SGD"
    params:
      lr: 0.1
      #lr_decay_epochs: [160, 180]
      #lr_decay_rate: 0.1
      momentum: 0.9
      weight_decay: 2e-4
      #disable_bias_weight_decay: True  # Disable weight decay regularization for biases
      #warmup_epochs: 5
  printing:
    print_training: True
    print_epoch_frequency: 1
    print_batch_frequency: 200
  plotting:
    draw_loss_plots: False
    plot_size:
      width: 8
      height: 6
    plot_path: "output/loss_plots/"
Evaluation:
  -
    method_name: "get_accuracy"
    method_params:
      calc_avg: True
      calc_perclass: True
      top: 1
      print_task_options: False